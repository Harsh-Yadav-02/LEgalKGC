import argparse
import ast
import csv
import json
import os
import re
import subprocess
import time
from collections import Counter
from tqdm import tqdm

def read_original_entity_csv(filepath):
    """
    Reads the CSV file generated by the extraction module.
    Expects columns "entity" and "entity_types" (a stringified list).
    Returns a dictionary mapping each entity to a list of candidate types.
    """
    entity_dict = {}
    with open(filepath, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            entity = row["entity"].strip()
            candidate_str = row["entity_types"].strip()
            try:
                candidates = ast.literal_eval(candidate_str)
                if isinstance(candidates, list):
                    entity_dict[entity] = candidates
                else:
                    entity_dict[entity] = [str(candidates)]
            except Exception as e:
                print(f"Error parsing candidate list for entity '{entity}': {e}")
                entity_dict[entity] = []
    return entity_dict

def load_document_text(document_file):
    """Load the full document text from a file."""
    with open(document_file, 'r', encoding='utf-8') as f:
        return f.read()

def split_document_into_paragraphs(document_text):
    """
    Splits the document text into paragraphs.
    Assumes paragraphs are separated by two newlines.
    """
    paragraphs = [para.strip() for para in document_text.split("\n\n") if para.strip()]
    return paragraphs

def get_entity_paragraphs(document_text, entity):
    """
    Returns a list of paragraphs from the document that contain the entity (case-insensitive).
    """
    paragraphs = split_document_into_paragraphs(document_text)
    entity_lower = entity.lower()
    matching = [para for para in paragraphs if entity_lower in para.lower()]
    return matching

def chunk_list(lst, chunk_size):
    """
    Splits a list into chunks of maximum size chunk_size.
    """
    for i in range(0, len(lst), chunk_size):
        yield lst[i:i+chunk_size]

def build_prompt_for_chunk(entity, candidate_types, paragraphs_chunk):
    """
    Build a prompt for a single entity using a chunk of paragraphs.
    The prompt instructs the LLM to choose one candidate type based on this context.
    """
    context = "\n\n".join(paragraphs_chunk)
    prompt = f"""You are a legal expert. You are provided with a chunk of paragraphs from a legal judgment where the entity appears, along with candidate entity types.
Your task is to analyze this context and select the single most appropriate entity type from the candidate list for the given entity.
IMPORTANT: Output strictly valid JSON. Do not output any chain-of-thought, <think> tags, or commentary.
The output must be a JSON object mapping the entity to its chosen type.

Context:
{context}

Entity: {entity}
Candidate Entity Types: {json.dumps(candidate_types)}

Revised Result:"""
    return prompt

def extract_json(text):
    """
    Use a regular expression to extract the first JSON object from the text.
    """
    pattern = r"(\{(?:.|\n)*\})"
    match = re.search(pattern, text)
    if match:
        return match.group(1)
    return None

def call_llm(prompt, timeout=300):
    """
    Call DeepSeek R1 14B via Ollama with the given prompt.
    Prints the prompt and the LLM output.
    Expects a strictly valid JSON response.
    """
    try:
        result = subprocess.run(
            ['ollama', 'run', 'deepseek-r1:14b'],
            input=prompt,
            text=True,
            capture_output=True,
            timeout=timeout,
            encoding='utf-8'
        )
        print("---------- LLM Prompt ----------")
        print(prompt)
        print("---------- LLM Output ----------")
        print(result.stdout)
        print("---------- LLM Error (if any) ----------")
        print(result.stderr)
        output = result.stdout.strip()
        try:
            return json.loads(output)
        except json.JSONDecodeError:
            extracted = extract_json(output)
            if extracted:
                try:
                    return json.loads(extracted)
                except json.JSONDecodeError:
                    return None
            else:
                return None
    except Exception as e:
        print("Exception while calling LLM:", str(e))
        return None

def majority_vote(decisions, candidate_order):
    """
    Given a list of decisions (each is one candidate type),
    returns the candidate with the highest count.
    In case of a tie, returns the first candidate according to candidate_order.
    """
    if not decisions:
        return None
    counter = Counter(decisions)
    max_count = max(counter.values())
    top_candidates = [cand for cand, count in counter.items() if count == max_count]
    for candidate in candidate_order:
        if candidate in top_candidates:
            return candidate
    return top_candidates[0]

def save_dict_as_csv(data, filepath, fieldnames):
    """Save a dictionary (keys and their associated value) as a CSV file."""
    out_dir = os.path.dirname(filepath)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir)
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for key, value in data.items():
            writer.writerow({fieldnames[0]: key, fieldnames[1]: value})

def main():
    start_time = time.time()
    parser = argparse.ArgumentParser(
        description="Disambiguate multi-candidate entities using context paragraphs and LLM majority voting."
    )
    parser.add_argument("--original_csv", type=str, required=True,
                        help="Path to the CSV file generated by the extraction module (with columns: entity, entity_types).")
    parser.add_argument("--document_file", type=str, required=True,
                        help="Path to the full document text file.")
    parser.add_argument("--output_json_file", type=str, default="final_entities.json",
                        help="Path for the final merged entity dictionary JSON file.")
    parser.add_argument("--output_csv_file", type=str, default="final_entities.csv",
                        help="Path for the final merged entity dictionary CSV file.")
    parser.add_argument("--chunk_paragraphs", type=int, default=5,
                        help="Maximum number of paragraphs to include in one LLM prompt chunk.")
    args = parser.parse_args()

    # Step 1: Read the original entities CSV.
    entity_dict = read_original_entity_csv(args.original_csv)
    print("Read original entities from CSV:")
    print(json.dumps(entity_dict, indent=2))

    # Step 2: Separate entities into:
    # - Single-candidate (accepted as-is)
    # - Multi-candidate (require disambiguation)
    final_entities = {}
    multi_candidate_entities = {}
    for entity, candidates in entity_dict.items():
        if len(candidates) == 1:
            final_entities[entity] = candidates[0]
        elif len(candidates) > 1:
            multi_candidate_entities[entity] = candidates
        else:
            print(f"Entity '{entity}' has no candidate types. Skipping.")

    print(f"Found {len(multi_candidate_entities)} multi-candidate entities needing disambiguation.")

    # Step 3: Load the full document text.
    document_text = load_document_text(args.document_file)

    # Step 4: Process each multi-candidate entity.
    skipped_count = 0
    for entity, candidates in tqdm(multi_candidate_entities.items(), desc="Disambiguating entities"):
        paragraphs = get_entity_paragraphs(document_text, entity)
        if not paragraphs:
            print(f"No paragraphs found for entity '{entity}'. Using fallback candidate: {candidates[0]}")
            final_entities[entity] = candidates[0]
            skipped_count += 1
            continue

        chunk_decisions = []
        for chunk in list(chunk_list(paragraphs, args.chunk_paragraphs)):
            prompt = build_prompt_for_chunk(entity, candidates, chunk)
            llm_response = call_llm(prompt)
            if (llm_response is None or not isinstance(llm_response, dict)
                    or entity not in llm_response or not llm_response[entity]):
                print(f"LLM call failed for entity '{entity}' on a chunk. Skipping this chunk.")
                continue
            decision = llm_response[entity]
            chunk_decisions.append(decision)

        if not chunk_decisions:
            print(f"No valid decisions from LLM for entity '{entity}'. Using fallback candidate: {candidates[0]}")
            final_entities[entity] = candidates[0]
            skipped_count += 1
        else:
            final_decision = majority_vote(chunk_decisions, candidates)
            final_entities[entity] = final_decision

    print(f"Total multi-candidate entities skipped or using fallback: {skipped_count}")

    # Step 5: Save the final merged entity dictionary as JSON and CSV.
    with open(args.output_json_file, "w", encoding='utf-8') as f:
        json.dump(final_entities, f, indent=2)
    print(f"Final entity dictionary saved to {args.output_json_file}")

    save_dict_as_csv(final_entities, args.output_csv_file, fieldnames=["entity", "final_type"])
    print(f"Final entity CSV saved to {args.output_csv_file}")

    print("---------- Final Merged Entity Dictionary ----------")
    print(json.dumps(final_entities, indent=2))

    elapsed = time.time() - start_time
    print(f"Total processing time: {elapsed:.2f} seconds")

if __name__ == "__main__":
    main()
# python postprocess_etypes.py --original_csv .output/original_entities.csv --document_file ./datasets/processed_cr.txt --output_json_file .output/final_entities.json --output_csv_file .output/final_entities.csv --chunk_paragraphs 5
